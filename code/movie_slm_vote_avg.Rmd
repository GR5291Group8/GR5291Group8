---
title: "movie_slm_sparsity_vote_avg"
author: 'Yan Qin (UNI: yq2232)'
date: "2018Äê10ÔÂ25ÈÕ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

data_new is defined in moive_slm_sparsity_language.

1 Solve for the sparsity of language matrix
```{r}
language_matrix  = data_1[, 27:81] # sparsity
language_num = colSums(language_matrix)
x = names(language_num)
y = as.numeric(language_num)
barplot(y, names.arg =x, main = "histogram of number of languages", xlab = "languages", ylab = "frequncy of languages in movies", ylim = c(0, 3000) ) 
sort(language_num, decreasing = TRUE)
summary(language_num)
# w set our threshold of being majority language as whose language_num greater than or equal to 3rd quantile of language_num.
lan_threshold = as.numeric(summary(language_num)[5])
lan_threshold # which is 28
# these are the majority languages
name = names(which(language_num>=lan_threshold))
language_reference_table[as.numeric(which(language_num>=lan_threshold)), 2]
major_language = language_matrix[, name]
# their numbers are
as.numeric(language_num[as.numeric(which(language_num>=lan_threshold))])
# minority language matrix
min_language = language_matrix[, which(!names(language_matrix)%in%name)]
min_language = rowSums(min_language)
major_language$min_language = as.numeric(min_language>0)
data_new = cbind(data_1[,1:26],major_language)
names(data_new)

```
data_new is our new data which combines all minority languages to one variable "min_language". 
We run linear model again.

2. Get ready for regression model
---------summary statistics
```{r}

data_vote_avg = data_new[, -c(6,7)]

# fit linear regression models
lmod = lm(vote_average~., data = data_vote_avg)


```
3. check for colinearity 
```{r}
library(car)
# vif
vif(lmod)
# all less than 10, no colinearity is detected
```
4 Separate data into Testing and Training datasets
```{r}
set.seed(123)
# 75% of the data are training; 25% of the data are testing
smp_size = floor(0.75 * nrow(data_vote_avg))
train_ind = sample(seq_len(nrow(data_vote_avg)), size = smp_size)
train = data_vote_avg[train_ind, ]
test = data_vote_avg[-train_ind, ]
```
5 Model Selection

(1)LASSO
```{r}
data(data_return_train); require(glmnet)
x = as.matrix(data_return_train[,-6])
y = data_return_train$return_rate
lmod_lasso = cv.glmnet(x, y=y, alpha = 1) 
plot(lmod_lasso)
# lambda that can minimize MSE
lmod_lasso$lambda.min
# coefficients of lasso predictors
coef(lmod_lasso, s=lmod_lasso$lambda.min)
```

(2) Stepwise Selection
```{r}
library(MASS)
lmod_train = lm(vote_average~., data = train)
stepAIC(lmod_train, direction="both")$anova
lmod_step_train = lm(vote_average ~ runtime + vote_count + year + month + Documentary + Crime + Foreign + Adventure + Western + Action + Comedy + Science.Fiction + Fantasy + Drama + Animation + Family + Horror + L4 + L9 + L12 + L15 + L50 + L53 + min_language, data = train)
summary(lmod_step_train) # for language, only "English" is selected in the model.
```
6 Diagnositics
(1) Linearity
```{r}
par(mfrow = c(2,2))
plot(lmod_step_train, 1)
summary(lmod_step_train)$r.squared
```
(2) Normality
```{r}
hist(lmod_step_train$residuals)
qqnorm(resid(lmod_step_train))
qqline(resid(lmod_step_train))
shapiro.test(lmod_step_train$residuals)# Not normal, but qqplot looks ok
```
(3) Homoscedasticiy
```{r}
# graph
par(mfrow = c(2,2))
plot(lmod_step_train)
library(lmtest)
# breush pagan test
bptest(lmod_step_train) # p-value < 0.05
# NCV Test
ncvTest(lmod_step_train) # p-value = 0.0122843 > 0.05
                         # constant variance
```
(4) Uncorrelated error
```{r}
durbinWatsonTest(lmod_step_train) # p = 0.606, errors are uncorrelated
```
(5) Outliers and influential points
```{r}
plot(cooks.distance(lmod_step_train))
plot(lmod_step_train, which = c(4))
# 2244, 2132, 182 are highly influential points
```
```{r}
# remove outlier
train_remove_out = train[-c(2244, 2132, 182),]

```
7 Transfromation
Log
```{r}
lmod_step_train_transform = lm(log(vote_average) ~ runtime + vote_count + year + month + Documentary + Crime + Foreign + Adventure + Western + Action + Comedy + Science.Fiction + Fantasy + Drama + Animation + Family + Horror + L4 + L9 + L12 + L15 + L50 + L53 + min_language, data = train_remove_out)
summary(lmod_step_train_transform)
```
Linearity
```{r}
par(mfrow = c(2,2))
plot(lmod_step_train_transform, 1)
summary(lmod_step_train_transform)$r.squared
```
Normality
```{r}
hist(lmod_step_train_transform$residuals)
qqnorm(resid(lmod_step_train_transform))
qqline(resid(lmod_step_train_transform))
shapiro.test(lmod_step_train_transform$residuals)
```
Homoscedasticity
```{r}
# graph
par(mfrow = c(2,2))
plot(lmod_step_train_transform)
# breush pagan test
bptest(lmod_step_train_transform) # p-value < 0.05
# NCV Test
ncvTest(lmod_step_train_transform) # p-value< 0.05
                                          # homoscedasticity fails
```
Uncorrelated Errors
```{r}
durbinWatsonTest(lmod_step_train_transform) # uncorrelated
```
We should not use log transformation
keep the original model
We should use lmod_step_train as our linear model (only normality fails)

8. Test Error
```{r}
lm_pred = predict(lmod_step_train,newdata=test[,-2])
error = mean(abs((test[,2]-lm_pred)/test[,2]), na.rm = TRUE)
```
