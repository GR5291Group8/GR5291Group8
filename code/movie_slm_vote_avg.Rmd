---
title: "movie_slm_sparsity_vote_avg"
author: 'Yan Qin (UNI: yq2232)'
date: "2018Äê10ÔÂ25ÈÕ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

data_new is defined in moive_slm_sparsity_language.

1 Solve for the sparsity of language matrix
```{r}
data_vote_avg = read.csv("cleaned_movie_data_vote_average.csv", header = TRUE)
```
data_new is our new data which combines all minority languages to one variable "min_language". 
We run linear model again.

2. Get ready for regression model
---------summary statistics
```{r}
# fit linear regression models
lmod = lm(vote_average~., data = data_vote_avg)


```
3. check for colinearity 
```{r}
library(car)
# vif
vif(lmod)
# all less than 10, no colinearity is detected
```
4 Separate data into Testing and Training datasets
```{r}
set.seed(123)
# 75% of the data are training; 25% of the data are testing
smp_size = floor(0.75 * nrow(data_vote_avg))
train_ind = sample(seq_len(nrow(data_vote_avg)), size = smp_size)
train = data_vote_avg[train_ind, ]
test = data_vote_avg[-train_ind, ]
```
5 Model Selection

(1)LASSO
```{r}
data(data_return_train); require(glmnet)
x = as.matrix(data_return_train[,-6])
y = data_return_train$return_rate
lmod_lasso = cv.glmnet(x, y=y, alpha = 1) 
plot(lmod_lasso)
# lambda that can minimize MSE
lmod_lasso$lambda.min
# coefficients of lasso predictors
coef(lmod_lasso, s=lmod_lasso$lambda.min)
```

(2) Stepwise Selection
```{r}
library(MASS)
lmod_train = lm(vote_average~., data = train)
stepAIC(lmod_train, direction="both")$anova
lmod_step_train = lm(vote_average ~ runtime + vote_count + year + month + Documentary + Crime + Foreign + Adventure + Western + Action + Comedy + Science.Fiction + Fantasy + Drama + Animation + Family + Horror + L4 + L9 + L12 + L15 + L50 + L53 + min_language, data = train)
summary(lmod_step_train) # for language, only "English" is selected in the model.
```
6 Diagnositics
(1) Linearity
```{r}
par(mfrow = c(2,2))
plot(lmod_step_train, 1)
summary(lmod_step_train)$r.squared
```
R^2 is 0.4778524

(2) Normality
```{r}
hist(lmod_step_train$residuals)
qqnorm(resid(lmod_step_train))
qqline(resid(lmod_step_train))
shapiro.test(lmod_step_train$residuals)
```
Not normal, but qqplot looks ok

(3) Homoscedasticiy
```{r}
# graph
par(mfrow = c(2,2))
plot(lmod_step_train)
library(lmtest)
# breush pagan test
bptest(lmod_step_train) # p-value < 0.05
# NCV Test
ncvTest(lmod_step_train) # p-value = 0.015 > 0.05
```
constant variance

(4) Uncorrelated error
```{r}
durbinWatsonTest(lmod_step_train) # p = 0.528
```
errors are uncorrelated

(5) Outliers and influential points
```{r}
plot(cooks.distance(lmod_step_train))
plot(lmod_step_train, which = c(4))
# 2244, 2132, 182 are highly influential points
```
```{r}
# remove outlier
train_remove_out = train[-c(2244, 2132, 182),]

```
7 Transfromation
Log
```{r}
lmod_step_train_transform = lm(log(vote_average) ~ runtime + vote_count + year + month + Documentary + Crime + Foreign + Adventure + Western + Action + Comedy + Science.Fiction + Fantasy + Drama + Animation + Family + Horror + L4 + L9 + L12 + L15 + L50 + L53 + min_language, data = train_remove_out)
summary(lmod_step_train_transform)
```
Linearity
```{r}
par(mfrow = c(2,2))
plot(lmod_step_train_transform, 1)
summary(lmod_step_train_transform)$r.squared
```
Normality
```{r}
hist(lmod_step_train_transform$residuals)
qqnorm(resid(lmod_step_train_transform))
qqline(resid(lmod_step_train_transform))
shapiro.test(lmod_step_train_transform$residuals)
```
Homoscedasticity
```{r}
# graph
par(mfrow = c(2,2))
plot(lmod_step_train_transform)
# breush pagan test
bptest(lmod_step_train_transform) # p-value < 0.05
# NCV Test
ncvTest(lmod_step_train_transform) # p-value< 0.05
```
! homoscedasticity fails

Uncorrelated Errors
```{r}
durbinWatsonTest(lmod_step_train_transform) # uncorrelated
```
We should not use log transformation
keep the original model
We should use lmod_step_train as our linear model (only normality fails)

8. Test Error
```{r}
lm_pred = predict(lmod_step_train,newdata=test[,-1])
error = mean(abs((test[,1]-lm_pred)/test[,1]), na.rm = TRUE)
```
