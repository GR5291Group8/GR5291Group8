---
title: "Untitled"
author: "xf2170"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
data = read.csv("cleaned_movie_data_vote_average.csv", header = TRUE)
```

```{r}
set.seed(123)
# 75% of the data are training; 25% of the data are testing
sample_size = floor(0.75 * nrow(data))
train_index = sample(seq_len(nrow(data)), size = sample_size)
train_data = data[train_index, ]
test_data = data[-train_index, ]
head(test_data)
```

```{r}
library(glmnet)
x = data.matrix(train_data[,-1])
y = train_data$vote_average
lambda_ridge = cv.glmnet(x, y = y, alpha = 0) 
ridge_model <- glmnet(x, y, alpha = 0, lambda = lambda_ridge$lambda.min)
coef(ridge_model)
```
Ridge model choose all 41 variables.

Diagnositics
The checks of several assumptions are still based on OLS linear regression model with all 41 variables.
(1) Linearity
```{r}
OLSmodel <- lm(vote_average ~ runtime + vote_count + year + month + History + Documentary + Crime + Thriller + Foreign + War + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L14 + L15 + L27 + L32 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train_data)
summary(OLSmodel)
plot(OLSmodel, 1)
summary(OLSmodel)$r.squared
```
R-squared is 0.483417
(2) Normality
```{r}
hist(OLSmodel$residuals)
qqnorm(resid(OLSmodel))
qqline(resid(OLSmodel))
shapiro.test(resid(OLSmodel))
```
The normality assumption is not satisfied.
(3) Homoscedasticiy
```{r}
# graph
par(mfrow = c(2,2))
plot(OLSmodel)
library(lmtest)
library(car)
bptest(OLSmodel) 
ncvTest(OLSmodel)
```
The assumption of homoscedasticiy is not satisfied.
(4) Uncorrelated error
```{r}
durbinWatsonTest(OLSmodel)
```
Errors are uncorrelated
(5) Outliers and influential points
```{r}
library(car)
outlierTest(OLSmodel)
summary(train_data$vote_average)
summary(train_data$runtime)
summary(train_data$vote_count)
summary(train_data$minority_studios)
summary(train_data$majority_studios)
train_data[row.names(train_data)=="706",]
train_data[row.names(train_data)=="2234",]
train_data[row.names(train_data)=="2581",]
train_data[row.names(train_data)=="357",]
# Outliers are films that made by small studios and have very small values of vote_average, and vote_count
plot(cooks.distance(OLSmodel))
plot(OLSmodel, which = c(4))
train_data[row.names(train_data)=="2244",] 
# lowest vote_average, very low runtime, almost lowest vote_count
train_data[row.names(train_data)=="2132",] 
# very low vote_average
train_data[row.names(train_data)=="182",] 
# almost highest vote_count, very high vote_average
# Influential points are films that made by small studios or have very small/high values of vote_average, and vote_count
```
Then remove outliers and influential points
```{r}
train_data_ro = train_data[!row.names(train_data)%in%c("706", "2234", "2581", "357", "2244", "2132", "182"),]
```
Use train dataset without outliers and influential points do OLS and ridge again
```{r}
OLSmodel_ro <- lm(vote_average ~ runtime + vote_count + year + month + History + Documentary + Crime + Thriller + Foreign + War + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L14 + L15 + L27 + L32 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train_data_ro)
library(glmnet)
x = data.matrix(train_data_ro[,-1])
y = train_data_ro$vote_average
lambda_ridge = cv.glmnet(x, y = y, alpha = 0) 
ridge_model_ro <- glmnet(x, y, alpha = 0, lambda = lambda_ridge$lambda.min)
selected_lambda <- lambda_ridge$lambda.min
selected_lambda
```
Use train data without outliers and influential points, ridge selected lambda = 0.03767885.
Test Error of OLS
```{r}
coef_OLS <- as.matrix(OLSmodel_ro$coefficients)[2:42]

predict_result_OLS <- as.matrix(test_data[,2:42]) %*% coef_OLS + OLSmodel_ro$coefficients[1]

R_squared_OLS <- sum((predict_result_OLS - test_data[,1]) * (predict_result_OLS - test_data[,1])) / 741 / var(test_data[,1])
R_squared_OLS

argmin_ridge_OLS <- sum((predict_result_OLS - test_data[,1]) * (predict_result_OLS - test_data[,1])) + selected_lambda * sum(coef_OLS * coef_OLS)
argmin_ridge_OLS
```
Test Error of ridge
```{r}
coef_ridge <- as.matrix(coef(ridge_model_ro)[2:42])

predict_result_ridge <- as.matrix(test_data[,2:42]) %*% coef_ridge + coef(ridge_model_ro)[1]

R_squared_ridge <- sum((predict_result_ridge - test_data[,1]) * (predict_result_ridge - test_data[,1])) / 741 / var(test_data[,1])
R_squared_ridge

argmin_ridge <- sum((predict_result_ridge - test_data[,1]) * (predict_result_ridge - test_data[,1])) + selected_lambda * sum(coef_ridge * coef_ridge)
argmin_ridge
```
Comparing test error of OLS and ridge, OLS has a better R-squared; however, ridge has a better result when considering the values of coefficients.
MAPE of ridge
```{r}
sum(abs((predict_result_ridge - test_data[,1]) / test_data[,1]))/741
```