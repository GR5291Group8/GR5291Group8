---
title: "movie_slm_sparsity_vote_avg"
author: 'Yan Qin (UNI: yq2232)'
date: "2018Äê10ÔÂ25ÈÕ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

data_new is defined in moive_slm_sparsity_language.

1 Solve for the sparsity of language matrix
```{r}
data_vote_avg = read.csv("~/5291/Project/cleaned_movie_data_vote_average.csv", header = TRUE)
```
data_new is our new data which combines all minority languages to one variable "min_language". 
We run linear model again.

2. Get ready for regression model
---------summary statistics
```{r}
# fit linear regression models
lmod = lm(vote_average~., data = data_vote_avg)


```
3. check for colinearity 
```{r}
library(car)
# vif
vif(lmod)
# all less than 10, no colinearity is detected
```
4 Separate data into Testing and Training datasets
```{r}
set.seed(123)
# 75% of the data are training; 25% of the data are testing
smp_size = floor(0.75 * nrow(data_vote_avg))
train_ind = sample(seq_len(nrow(data_vote_avg)), size = smp_size)
train = data_vote_avg[train_ind, ]
test = data_vote_avg[-train_ind, ]
head(test)
```
5 Model Selection

LASSO
```{r}
data(train); require(glmnet)
x = data.matrix(train[,-1])
y = train$vote_average
lmod_lasso = cv.glmnet(x, y=y, alpha = 1) 
plot(lmod_lasso)
# lambda that can minimize MSE
lmod_lasso$lambda.min
# coefficients of lasso predictors
#coef(lmod_lasso, s=lmod_lasso$lambda.min)
lasso_model <- glmnet(x,y,alpha = 1,lambda = lmod_lasso$lambda.min)
coef(lasso_model)
```


6 Diagnositics
(1) Linearity
```{r}
lassolm <- lm(vote_average ~ runtime + vote_count + year + month + Documentary + Crime + War + Foreign + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L15 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train)
par(mfrow = c(2,2))
plot(lassolm, 1)
summary(lassolm)$r.squared
```
R^2 is 0.4778524


(2) Normality


```{r}
hist(lassolm$residuals)
qqnorm(resid(lassolm))
qqline(resid(lassolm))
shapiro.test(resid(lassolm)) # p<0.05 fail
```

p-values both <0.05, fail

Not normal, but qqplot looks ok

(3) Homoscedasticiy
```{r}
# graph
par(mfrow = c(2,2))
plot(lassolm)
library(lmtest)
# breush pagan test
bptest(lassolm) # p-value < 0.05 fail
# NCV Test
ncvTest(lassolm) # p-value = 0.024 < 0.05 fail
```
p_value <0.05, fail

(4) Uncorrelated error
```{r}
durbinWatsonTest(lassolm) # p = 0.604 > 0.05, hold
```
errors are uncorrelated

(5) Outliers and influential points
```{r}
library(car)
# outlier
outlierTest(lassolm)
# look at the data 
summary(train$vote_average);summary(train$runtime);summary(train$vote_count)
summary(train$minority_studios);summary(train$majority_studios)
train[row.names(train)=="706",]
train[row.names(train)=="2234",]
train[row.names(train)=="2581",]
train[row.names(train)=="357",]
# outliers are films that made by small studios and have very small values of vote_average, and vote_count
# we can remove them 

#  Cook's distance is higher than 1 are to be considered as influential
plot(cooks.distance(lassolm))
plot(lassolm, which = c(4))
# 2244, 2132, 182 are suspected to be influential points 
# look at the data
train[row.names(train)=="2244",] # lowest vote_average, very low runtime, almost lowest vote_count
train[row.names(train)=="2132",] # very low vote_average
train[row.names(train)=="182",] # almost highest vote_count, very high vote_average

# influential points are films that made by small studios or have very small/high values of vote_average, and vote_count
# they doesn't respresent majority of films we can remove them
```

```{r}
# remove outlier
train_remove_out = train[!row.names(train)%in%c("706", "2234", "2581", "357", "2244", "2132", "182"),]
```
7 Transfromation
Log
```{r}
lmod_step_train_transform = lm(log(vote_average) ~ runtime + vote_count + year + month + Documentary + Crime + Foreign + Adventure + Western + Action + Comedy + Science.Fiction + Fantasy + Drama + Animation + Family + Horror + L4 + L9 + L12 + L15 + L50 + L53 + min_language, data = train_remove_out)
summary(lmod_step_train_transform)
```
Linearity
```{r}
par(mfrow = c(2,2))
plot(lasso_transform, 1)
summary(lasso_transform)$r.squared
```
R^2 is 0.4387728

Normality

```{r}
lasso_transform = lm(log(vote_average) ~ runtime + vote_count + year + month + Documentary + Crime + War + Foreign + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L15 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train)
lasso_transform
hist(lasso_transform$residuals)
qqnorm(resid(lasso_transform))
qqline(resid(lasso_transform))
shapiro.test(resid(lasso_transform)) #p < 0.05, fail
```
p_value too small, fail

Homoscedasticity
```{r}
# graph
par(mfrow = c(2,2))
plot(lasso_transform)
# breush pagan test
bptest(lasso_transform) # p-value < 0.05
# NCV Test
ncvTest(lasso_transform) # p-value< 0.05
```
P-value <0.05,
homoscedasticity fails

Uncorrelated Errors
```{r}
durbinWatsonTest(lasso_transform) # uncorrelated
```
We should not use log transformation
keep the original model
We should use lassolm as our linear model (only normality fails)



7.2 Boxcox
```{r}
library(caret)
dist_boxcox = BoxCoxTrans(train$vote_average)
dist_new = predict(dist_boxcox, train$vote_average)
fit_train_boxcox = lm(dist_new ~ runtime + vote_count + year + month + Documentary + Crime + War + Foreign + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L15 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train)
# Linearity
par(mfrow = c(2,2))
plot(fit_train_boxcox, 1)
summary(fit_train_boxcox)$r.squared
# r_sqaured = 0.58

# Normality
hist(fit_train_boxcox$residuals)
qqnorm(resid(fit_train_boxcox))
qqline(resid(fit_train_boxcox)) # qqplot looks better than log transformed model and original model
shapiro.test(fit_train_boxcox$residuals) # p-value < 0.05

# Equal variance
bptest(fit_train_boxcox)
ncvTest(fit_train_boxcox) # p <0.05 still fail

# Uncorrelated error
durbinWatsonTest(fit_train_boxcox) # uncorrelated error
```

Since homoscedasticity fails everytime, we try weighted least squares model
7.3 WLS
```{r}
lmod_remove_out =  lm(vote_average~  runtime + vote_count + year + month + Documentary + Crime + War + Foreign + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L15 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, data = train_remove_out)

# Fit a WLS model using weights = 1/(fitted values)2.
wts = 1/fitted(lm(abs(residuals(lmod_remove_out)) ~  fitted(lmod_remove_out), data = train_remove_out))^2
# Weighted Least Squares model
wlsmod = lm(vote_average ~ runtime + vote_count + year + month + Documentary + Crime + War + Foreign + Adventure + Western + Music + Mystery + Action + Comedy + Science.Fiction + Romance + Fantasy + Drama + Animation + Family + Horror + L4 + L6 + L9 + L10 + L12 + L13 + L15 + L33 + L50 + L52 + L53 + L54 + min_language + majority_studios + minority_studios, weights = wts, data = train_remove_out)

# check assumptions
# Linearity
par(mfrow = c(2,2))
plot(wlsmod, 1)
summary(wlsmod)$r.squared
# r_sqaured = 0.5025

# Normality
hist(wlsmod$residuals)
qqnorm(resid(wlsmod))
qqline(resid(wlsmod)) # qqplot looks better than log transformed model and original model


# Equal variance
bptest(wlsmod) # p-value < 0.05
ncvTest(wlsmod) # p-value = 0.650> 0.05 homoscedasticity

# Uncorrelated error
durbinWatsonTest(wlsmod) # p-value = 0.982; uncorrelated error
```






8. Test Error
```{r}
testdata <- data.matrix(test[,-1])
lm_pred_glmnet <- predict(lasso_model,newx = testdata) # directly using glmnet,indeed like Ols traing with var selected
error_glmnet <- mean(abs((test[,1]-lm_pred)/test[,1]), na.rm = TRUE)
lm_pred_wls <- predict(wlsmod,newx = testdata)# by WLS
error_wls <- mean(abs((test[,1]-lm_pred)/test[,1]), na.rm = TRUE)
error_glmnet
error_wls
```





####
binary variables too many, not continous, not local-best solution